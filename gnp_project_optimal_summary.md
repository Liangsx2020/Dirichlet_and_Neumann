# GNP (Graph Neural Preconditioner) ä¼˜åŒ–é¡¹ç›®æ€»ç»“

## ğŸ“‹ é¡¹ç›®èƒŒæ™¯

### é—®é¢˜æè¿°
- **ç›®æ ‡**: ä½¿ç”¨Graph Neural Network Preconditioners (GNP) åŠ é€ŸGMRESæ±‚è§£å™¨è§£å†³2D Poissonæ–¹ç¨‹
- **æ ¸å¿ƒæŒ‘æˆ˜**: GNPåœ¨å°é—®é¢˜(n=8)æœ‰æ•ˆï¼Œä½†éšç€é—®é¢˜å°ºå¯¸å¢å¤§åˆ°n=64æ—¶å¤±æ•ˆ
- **ç ”ç©¶æ–¹æ³•**: é€šè¿‡è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œç³»ç»Ÿæ€§è¶…å‚æ•°æœç´¢ï¼Œè¯†åˆ«å…³é”®ç“¶é¢ˆå¹¶æå‡ºæ”¹è¿›æ–¹æ¡ˆ

### é—®é¢˜è®¾ç½®
- **æ–¹ç¨‹**: 2D Poissonæ–¹ç¨‹ âˆ‡Â²p = fï¼Œæ··åˆDirichlet/Neumannè¾¹ç•Œæ¡ä»¶
- **ç¦»æ•£åŒ–**: æœ‰é™å·®åˆ†æ³•ï¼ŒnÃ—nç½‘æ ¼ï¼ŒçŸ©é˜µå°ºå¯¸nÂ²Ã—nÂ²
- **æ±‚è§£å™¨**: GMRES with restart
- **é¢„æ¡ä»¶å­**: ResGCNç½‘ç»œå­¦ä¹ A^(-1)çš„è¿‘ä¼¼

## ğŸ¯ è´å¶æ–¯ä¼˜åŒ–å…³é”®å‘ç°

### æœ€å…³é”®è¶…å‚æ•°æ’åºï¼ˆæŒ‰é‡è¦æ€§ï¼‰

1. **`training_data = 'x_normal'`** â­â­â­â­â­
   - **å‘ç°**: æ¯”'x_subspace'å’Œ'x_mix'æ•ˆæœæ˜¾è‘—æ›´å¥½
   - **åŸå› **: ç®€å•çš„æ­£æ€åˆ†å¸ƒéšæœºå‘é‡æœ€æœ‰æ•ˆè¦†ç›–è§£ç©ºé—´
   - **å½±å“**: è¿™æ˜¯æˆè´¥çš„å†³å®šæ€§å› ç´ 

2. **`num_layers = 3`** â­â­â­â­
   - **å‘ç°**: æµ…å±‚ç½‘ç»œæ¯”æ·±å±‚ç½‘ç»œ(8-12å±‚)æ•ˆæœæ›´å¥½
   - **åŸå› **: æ·±å±‚ç½‘ç»œå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œæµ…å±‚ç½‘ç»œæ³›åŒ–èƒ½åŠ›æ›´å¼º
   - **åç›´è§‰**: æ›´å¤æ‚çš„ç½‘ç»œåè€Œæ€§èƒ½æ›´å·®

3. **`lr = 0.008861` (çº¦0.009)** â­â­â­â­
   - **å‘ç°**: æ¯”å¸¸ç”¨å­¦ä¹ ç‡(1e-3)é«˜å¾ˆå¤š
   - **åŸå› **: GNPéœ€è¦å¿«é€Ÿå­¦ä¹ é€†ç®—å­ï¼Œéœ€è¦æ›´aggressiveçš„æ›´æ–°
   - **å…³é”®**: å­¦ä¹ ç‡å¯¹è®­ç»ƒæ”¶æ•›è‡³å…³é‡è¦

4. **`embed = 61`** â­â­â­
   - **å‘ç°**: ä¸­ç­‰åµŒå…¥ç»´åº¦æœ€ä¼˜ï¼Œä¸éœ€è¦å¾ˆå¤§çš„ç»´åº¦
   - **å¹³è¡¡**: è¡¨è¾¾èƒ½åŠ› vs è¿‡æ‹Ÿåˆé£é™©
   - **ç»æµæ€§**: 61ç»´æä¾›æœ€ä½³æ€§ä»·æ¯”

5. **`drop_rate = 0.07`** â­â­
   - **å‘ç°**: è½»å¾®dropoutæœ‰å¸®åŠ©ï¼Œè¿‡å¼ºdropout(0.2+)æœ‰å®³
   - **ä½œç”¨**: é€‚åº¦æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
   - **ç²¾ç¡®æ€§**: éœ€è¦ç²¾ç¡®æ§åˆ¶å¼ºåº¦

6. **`m = 62`** â­â­
   - **å‘ç°**: ä¸­ç­‰Krylovå­ç©ºé—´ç»´åº¦
   - **ä½œç”¨**: å½±å“è®­ç»ƒæ•°æ®è´¨é‡
   - **éœ€è¦**: éšé—®é¢˜å°ºå¯¸é€‚åº”æ€§è°ƒæ•´

### æ¬¡è¦ä½†æœ‰ç”¨çš„å‚æ•°
- `hidden = 122` (embed * 2)
- `epochs = 322` (adaptive: 200 + embed * 2)
- `batch_size = 8`
- `weight_decay = 1e-5`
- `scale_input = True`

## ğŸ“Š æ€§èƒ½éªŒè¯ç»“æœ

### å®æµ‹æ€§èƒ½è¡¨ç°

| é—®é¢˜å°ºå¯¸ | çŸ©é˜µå¤§å° | Speedup | è¿­ä»£å‡å°‘ | è®­ç»ƒLoss | æ˜¯å¦æœ‰æ•ˆ |
|---------|---------|---------|----------|----------|----------|
| n=16    | 256Ã—256 | 0.72x   | 55.3%    | 8.32e-02 | âŒ |
| n=24    | 576Ã—576 | 1.40x   | 76.8%    | 7.43e-02 | âœ… |
| n=32    | 1024Ã—1024| 1.76x   | 79.7%    | 6.76e-02 | âœ… |

### å…³é”®è§‚å¯Ÿ
- **æˆåŠŸç‡**: 66.7% (2/3)
- **å°ºå¯¸æ•ˆåº”**: å¯¹ä¸­å¤§å‹é—®é¢˜(nâ‰¥24)éå¸¸æœ‰æ•ˆ
- **è®­ç»ƒè´¨é‡**: Lossä»~1.0é™åˆ°~0.07ï¼Œæ”¶æ•›è‰¯å¥½
- **è¿­ä»£æ•ˆç‡**: èƒ½å‡å°‘70-80%çš„GMRESè¿­ä»£æ•°

## ğŸ” n=64å¤±æ•ˆåŸå› åˆ†æ

### ä¸»è¦ç“¶é¢ˆ

1. **è®­ç»ƒæˆæœ¬ vs æ±‚è§£æ”¶ç›Šå¤±è¡¡**
   - çŸ©é˜µå°ºå¯¸: 4096Ã—4096 (æ¯”n=32å¤§16å€)
   - è®­ç»ƒæ—¶é—´å¢é•¿: å¯èƒ½éœ€è¦8-15ç§’
   - åŸºç¡€GMRESæ—¶é—´: 2-8ç§’
   - **é—®é¢˜**: è®­ç»ƒæ—¶é—´å¢é•¿è¶…è¿‡æ±‚è§£æ—¶é—´èŠ‚çœ

2. **ç½‘ç»œå®¹é‡ä¸é—®é¢˜å¤æ‚åº¦ä¸åŒ¹é…**
   - å½“å‰é…ç½®: embed=61, layers=3
   - n=64å¤æ‚åº¦: æ˜¯n=32çš„4-16å€
   - **å®¹é‡ä¸è¶³**: ç½‘ç»œæ— æ³•å……åˆ†å­¦ä¹ å¤§çŸ©é˜µé€†ç®—å­

3. **æ•°å€¼ç¨³å®šæ€§æ¶åŒ–**
   - æ¡ä»¶æ•°å¢é•¿: O(nÂ²) â‰ˆ O(4096)
   - ç²¾åº¦é—®é¢˜: float32å¯èƒ½ä¸è¶³
   - è¯¯å·®ç´¯ç§¯: å¤§çŸ©é˜µä¸Šè¿‘ä¼¼è¯¯å·®æ”¾å¤§

4. **è¶…å‚æ•°å°ºå¯¸ä¾èµ–æ€§**
   - å½“å‰"æœ€ä¼˜"å‚æ•°é’ˆå¯¹n=16-32ä¼˜åŒ–
   - å¤§é—®é¢˜éœ€è¦ä¸åŒçš„å‚æ•°é…ç½®
   - ç¼ºä¹è‡ªé€‚åº”æœºåˆ¶

## ğŸš€ ä¸‰å±‚æ¬¡æ”¹è¿›æ–¹æ¡ˆ

### Level 1: ç«‹å³å¯æ“ä½œæ”¹è¿›

#### 1.1 è‡ªé€‚åº”è¶…å‚æ•°ç­–ç•¥
```python
def get_adaptive_config(n):
    scale_factor = (n / 16) ** 0.5  # å¹³æ–¹æ ¹ç¼©æ”¾
    
    return {
        'embed': min(200, int(61 * scale_factor)),
        'num_layers': min(8, int(3 + np.log2(n/16))),
        'm': min(150, int(62 * scale_factor)),
        'epochs': min(800, int(322 + (n-16) * 15)),
        'lr': 8.9e-3 * (16/n)**0.25,  # å¤§é—®é¢˜ç”¨ç¨å°å­¦ä¹ ç‡
        'training_data': 'x_normal',   # ä¿æŒæœ€ä½³è®¾ç½®
        'drop_rate': 0.07,
        'scale_input': True
    }
```

#### 1.2 æ··åˆç²¾åº¦è®­ç»ƒ
```python
# ç½‘ç»œç”¨float32ï¼Œå…³é”®è®¡ç®—ç”¨float64
b_out = (self.A @ x_out.to(torch.float64)).to(self.dtype)
loss = F.mse_loss(x_out, x) if self.training_data != 'no_x' else F.mse_loss(b_out, b)
```

#### 1.3 å¢å¼ºè®­ç»ƒæ•°æ®
```python
training_data_mix = {
    'x_normal': 0.4,      # 40% æ­£æ€éšæœº
    'x_lowfreq': 0.3,     # 30% ä½é¢‘åˆ†é‡  
    'x_subspace': 0.2,    # 20% Krylovå­ç©ºé—´
    'x_smooth': 0.1       # 10% å¹³æ»‘å‡½æ•°
}
```

### Level 2: ä¸­æœŸç®—æ³•æ”¹è¿›

#### 2.1 æ®‹å·®é¢„æ¡ä»¶å­æ¶æ„ â­ (æœ€æ¨è)
```python
class ResidualGNP(nn.Module):
    def __init__(self, A):
        self.A = A
        self.jacobi_precond = torch.diag(1.0 / torch.diag(A))  # é›…å¯æ¯”é¢„æ¡ä»¶å­
        self.neural_correction = ResGCN(...)  # ç¥ç»ç½‘ç»œä¿®æ­£
        
    def apply(self, r):
        # M = M_jacobi + Î”M_neural
        z_jacobi = self.jacobi_precond @ r
        delta_z = self.neural_correction(r)
        return z_jacobi + delta_z
```

#### 2.2 å±‚æ¬¡åŒ–è®­ç»ƒç­–ç•¥
```python
# æ¸è¿›å¼è®­ç»ƒ: n=16 â†’ n=32 â†’ n=64
# 1. åœ¨å°é—®é¢˜ä¸Šé¢„è®­ç»ƒ
# 2. å›ºå®šæµ…å±‚ï¼Œåœ¨ä¸­ç­‰é—®é¢˜ä¸Šå¾®è°ƒæ·±å±‚  
# 3. å…¨ç½‘ç»œåœ¨å¤§é—®é¢˜ä¸Šç»†è°ƒ
def hierarchical_training(sizes=[16, 32, 64]):
    for i, n in enumerate(sizes):
        if i == 0:
            # å…¨ç½‘ç»œè®­ç»ƒ
            train_full_network(n)
        else:
            # å›ºå®šå‰å‡ å±‚ï¼Œåªè®­ç»ƒåå‡ å±‚
            freeze_early_layers()
            train_later_layers(n)
```

#### 2.3 æ³¨æ„åŠ›å¢å¼ºGCN
```python
class AttentionGCN(nn.Module):
    def __init__(self):
        self.local_gcn = GCNConv(...)
        self.global_attention = MultiHeadAttention(...)
        
    def forward(self, x):
        local_feat = self.local_gcn(x)
        global_feat = self.global_attention(local_feat)
        return local_feat + global_feat  # æ®‹å·®è¿æ¥
```

#### 2.4 ç‰©ç†çº¦æŸæŸå¤±
```python
def physics_informed_loss(x_pred, b, A):
    # æ ‡å‡†é¢„æ¡ä»¶å­æŸå¤±
    precon_loss = F.mse_loss(x_pred, x_true)
    
    # ç‰©ç†çº¦æŸ: Ax = b
    physics_loss = F.mse_loss(A @ x_pred, b)
    
    # è¾¹ç•Œæ¡ä»¶çº¦æŸ
    boundary_loss = enforce_boundary_conditions(x_pred)
    
    return precon_loss + Î»1 * physics_loss + Î»2 * boundary_loss
```

### Level 3: æ ¹æœ¬æ€§åˆ›æ–°

#### 3.1 å¤šå°ºåº¦GNP
```python
class MultiScaleGNP:
    def __init__(self, levels=[16, 32, 64]):
        self.gnp_coarse = ResGCN(...)   # ç²—ç½‘æ ¼GNP
        self.gnp_fine = ResGCN(...)     # ç»†ç½‘æ ¼GNP
        self.restrict = ...             # é™åˆ¶ç®—å­
        self.interpolate = ...          # æ’å€¼ç®—å­
        
    def apply(self, r):
        # V-cycleç±»ä¼¼å¤šé‡ç½‘æ ¼
        r_coarse = self.restrict(r)
        z_coarse = self.gnp_coarse(r_coarse)
        z_fine = self.interpolate(z_coarse)
        
        # ç»†ç½‘æ ¼ä¿®æ­£
        residual = r - self.A @ z_fine
        correction = self.gnp_fine(residual)
        
        return z_fine + correction
```

#### 3.2 è‡ªé€‚åº”ç½‘ç»œç”Ÿé•¿
```python
class AdaptiveGNP:
    def grow_network_if_needed(self):
        if self.training_loss_plateau():
            if self.current_capacity < self.problem_complexity:
                self.add_layer() or self.increase_width()
                
    def training_loss_plateau(self):
        return np.std(self.loss_history[-50:]) < 1e-6
```

#### 3.3 é¢†åŸŸåˆ†è§£+GNP
```python
class DomainDecompositionGNP:
    def __init__(self, num_subdomains=4):
        self.subdomains = self.partition_domain(num_subdomains)
        self.sub_gnps = [ResGCN(...) for _ in range(num_subdomains)]
        
    def apply(self, r):
        # 1. åˆ†è§£åˆ°å„å­åŸŸ
        sub_residuals = self.distribute_to_subdomains(r)
        
        # 2. å„å­åŸŸå¹¶è¡ŒGNPæ±‚è§£
        sub_solutions = [gnp.apply(sub_r) for gnp, sub_r in 
                        zip(self.sub_gnps, sub_residuals)]
        
        # 3. ç»„åˆè§£å¹¶å¤„ç†æ¥å£
        return self.combine_subdomain_solutions(sub_solutions)
```

## ğŸ“… æ¨èå®æ–½è·¯å¾„

### çŸ­æœŸç›®æ ‡ (1-2å‘¨)
1. **å®æ–½Level 1.1å’Œ1.2**: è‡ªé€‚åº”å‚æ•° + æ··åˆç²¾åº¦
2. **éªŒè¯n=64æ•ˆæœ**: æµ‹è¯•åŸºç¡€æ”¹è¿›æ˜¯å¦è¶³å¤Ÿ
3. **å¦‚æœä»ä¸å¤Ÿ**: è¿›å…¥ä¸­æœŸæ”¹è¿›

### ä¸­æœŸç›®æ ‡ (1ä¸ªæœˆ)  
4. **å®æ–½Level 2.1**: æ®‹å·®é¢„æ¡ä»¶å­æ¶æ„ (æœ€ä¼˜å…ˆ)
5. **å®æ–½Level 2.2**: å±‚æ¬¡åŒ–è®­ç»ƒç­–ç•¥
6. **å¯¹æ¯”åˆ†æ**: æ‰¾åˆ°æœ€æœ‰æ•ˆçš„æ”¹è¿›ç»„åˆ

### é•¿æœŸç›®æ ‡ (2-3ä¸ªæœˆ)
7. **å¦‚æœéœ€è¦**: å®æ–½Level 3çš„æ ¹æœ¬åˆ›æ–°
8. **æ‰©å±•æµ‹è¯•**: n=128, 256ç­‰æ›´å¤§é—®é¢˜
9. **æ–¹æ³•æ€»ç»“**: å‘è¡¨æ”¹è¿›æ–¹æ³•è®ºæ–‡

## ğŸ¯ å…³é”®é…ç½®è®°å½•

### å½“å‰æœ€ä½³é…ç½®
```python
best_config = {
    'training_data': 'x_normal',
    'm': 62,
    'embed': 61, 
    'num_layers': 3,
    'lr': 0.008861577452533074,
    'drop_rate': 0.06983140212909128,
    'hidden': 122,
    'epochs': 322,
    'batch_size': 8,
    'weight_decay': 1e-5,
    'scale_input': True
}
```

### è´å¶æ–¯ä¼˜åŒ–æœç´¢ç©ºé—´
```python
search_space = [
    Categorical(['x_normal', 'x_subspace', 'x_mix'], name='training_data'),
    Integer(20, 100, name='m'),
    Integer(16, 128, name='embed'), 
    Integer(3, 12, name='num_layers'),
    Real(-4.0, -2.0, name='lr_log'),  # 1e-4 to 1e-2
    Real(0.0, 0.3, name='drop_rate'),
]
```

## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿæ€»ç»“

### æˆåŠŸå› ç´ 
1. **è®­ç»ƒæ•°æ®è´¨é‡å†³å®šä¸€åˆ‡**: x_normal >> x_subspace/x_mix
2. **ç®€å•æ¶æ„æ›´å¥å£®**: æµ…å±‚ç½‘ç»œèƒœè¿‡æ·±å±‚ç½‘ç»œ
3. **å­¦ä¹ ç‡éœ€è¦è¶³å¤Ÿaggressive**: ~0.009 >> 0.001
4. **é—®é¢˜å°ºå¯¸æœ‰æ˜æ˜¾threshold**: nâ‰¥24æ‰å¼€å§‹æœ‰æ•ˆ

### å¤±è´¥æ•™è®­
1. **ä¸è¦ç›²ç›®å¢åŠ ç½‘ç»œå¤æ‚åº¦**: æ›´æ·±æ›´å®½ä¸ç­‰äºæ›´å¥½
2. **ä¼ ç»ŸMLç»éªŒä¸å®Œå…¨é€‚ç”¨**: GNPæœ‰å…¶ç‹¬ç‰¹è§„å¾‹
3. **å°ºå¯¸scalingéœ€è¦ç‰¹æ®Šè€ƒè™‘**: ä¸èƒ½ç®€å•å¤–æ¨å°é—®é¢˜ç»éªŒ

### æœªæ¥æ–¹å‘
1. **æ®‹å·®æ¶æ„æœ€æœ‰å‰æ™¯**: ç»“åˆä¼ ç»Ÿæ–¹æ³•ä¼˜åŠ¿
2. **è‡ªé€‚åº”æœºåˆ¶æ˜¯å…³é”®**: å‚æ•°éœ€è¦éšé—®é¢˜è°ƒæ•´
3. **å¤šå°ºåº¦æ€æƒ³å€¼å¾—æ¢ç´¢**: å€Ÿé‰´å¤šé‡ç½‘æ ¼æˆåŠŸç»éªŒ

## ğŸ“ ç›¸å…³æ–‡ä»¶
- `bayesian_hyperparam_search.py`: è´å¶æ–¯ä¼˜åŒ–å®ç°
- `analyze_best_config.py`: æœ€ä½³é…ç½®éªŒè¯
- `simple.py`: ä¸»è¦å®éªŒè„šæœ¬
- `GNP/nn/ResGCN.py`: ç½‘ç»œæ¶æ„å®šä¹‰
- `GNP/preconditioners/GNP.py`: é¢„æ¡ä»¶å­å®ç°

---
**é¡¹ç›®çŠ¶æ€**: å·²å®Œæˆè¶…å‚æ•°ä¼˜åŒ–ï¼Œå‡†å¤‡å®æ–½ç®—æ³•æ”¹è¿›
**ä¸‹ä¸€æ­¥**: ä¼˜å…ˆå®æ–½æ®‹å·®é¢„æ¡ä»¶å­æ¶æ„ (Level 2.1)
**æœ€ç»ˆç›®æ ‡**: ä½¿GNPåœ¨n=64åŠä»¥ä¸Šé—®é¢˜ä¸Šè¶…è¶Šä¼ ç»ŸGMRES 